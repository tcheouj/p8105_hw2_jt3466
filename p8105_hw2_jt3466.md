p8105_hw2_jt3466
================
Johnstone Tcheou
2024-09-24

# Question 1

This code reads in the NYC Transit Subway data, cleans the variable
names, and retains only the needed variables - `line`, `station_name`,
`station_latitude`, `station_longitude`, `route1:11`, `entry`,
`vending`, `entrance_type`, and `ada`. In order to pivot the data to a
longer format, all route variables need to be the same variable type, so
they are converted to character type. Afterwards, all rows with `NA` in
in subway lines are dropped, since not all stations have 11 routes. The
`entry` variable is then converted from character to logical type.
`vending` is also converted from character to logical for a later
question.

``` r
subway <- read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") |>
  janitor::clean_names() |>
  select(
    station_name, 
    division, 
    station_latitude, 
    station_longitude, 
    route1:route11, 
    entry, 
    vending, 
    entrance_type, 
    ada
  ) |>
  mutate(
    route8 = as.character(route8),
    route9 = as.character(route9),
    route10 = as.character(route10),
    route11 = as.character(route11)
  ) |>
  pivot_longer(
    cols = route1:route11, 
    names_to = "route",
    names_prefix = "route",
    values_to = "subway_line"
  ) |> 
  drop_na(subway_line) |>
  mutate(
    entry = case_match(
      entry, 
      "YES" ~ TRUE,
      "NO" ~ FALSE
    ), 
    vending = case_match(
      vending, 
      "YES" ~ TRUE,
      "NO" ~ FALSE
    )
  )
```

    ## Rows: 1868 Columns: 32
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (22): Division, Line, Station Name, Route1, Route2, Route3, Route4, Rout...
    ## dbl  (8): Station Latitude, Station Longitude, Route8, Route9, Route10, Rout...
    ## lgl  (2): ADA, Free Crossover
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

## How many distinct stations are there?

There are 356 distinct stations.

## How many stations are ADA compliant?

1616 stations are ADA compliant.

## What proportion of station entrances/exits without vending allow entrance?

Of the 447 stations without vending, 31.0961969% of them allow entrance.

## How many distinct stations serve the A train?

56 distinct stations serve the A train.

## Of the stations that serve the A train, how many are ADA compliant?

Of the 56 stations that serve the A train, 16 stations are ADA
compliant.

# Question 2

This code chunk imports the corresponding cell ranges and sheets for the
3 robots, `mr_trash`, `prof_trash`, and `gwynnda`. All dataframes are
read in with `read_excel`, had names cleaned using `clean_names`. A
variable, `robot`, was created for all dataframes to clearly show which
rows belong to which robot. In order to bind rows, the `year` variable
from the `mr_trash` dataframe needed to be explicitly coerced to
`double` type to match the other `year` variables from the other
dataframes.

``` r
mr_trash <- read_excel("data/202309 Trash Wheel Collection Data.xlsx", range = "A2:N586", sheet = "Mr. Trash Wheel") |>
  janitor::clean_names() |>
  mutate(
    sports_balls = as.integer(round(sports_balls, digits = 0)),
    year = as.double(year),
    robot = "Mr. Trash Wheel"
  )

prof_trash <- read_excel("data/202309 Trash Wheel Collection Data.xlsx", range = "A2:M108", sheet = "Professor Trash Wheel") |>
  janitor::clean_names() |>
  mutate(
    robot = "Professor Trash Wheel"
  )

gwynnda <- read_excel("data/202309 Trash Wheel Collection Data.xlsx", range = "A2:L157", sheet = "Gwynnda Trash Wheel") |>
  janitor::clean_names() |>
  mutate(
    robot = "Gwynnda"
  )

trash_wheels <- bind_rows(mr_trash, prof_trash, gwynnda)

summary(trash_wheels)
```

    ##     dumpster      month                year     
    ##  Min.   :  1   Length:845         Min.   :2014  
    ##  1st Qu.: 71   Class :character   1st Qu.:2017  
    ##  Median :162   Mode  :character   Median :2019  
    ##  Mean   :223                      Mean   :2019  
    ##  3rd Qu.:373                      3rd Qu.:2021  
    ##  Max.   :584                      Max.   :2023  
    ##                                                 
    ##       date                         weight_tons    volume_cubic_yards
    ##  Min.   :1900-01-20 00:00:00.00   Min.   :0.610   Min.   : 5.00     
    ##  1st Qu.:2017-06-21 00:00:00.00   1st Qu.:2.490   1st Qu.:15.00     
    ##  Median :2019-10-25 00:00:00.00   Median :3.070   Median :15.00     
    ##  Mean   :2019-06-08 04:53:06.75   Mean   :3.009   Mean   :15.13     
    ##  3rd Qu.:2021-11-04 00:00:00.00   3rd Qu.:3.540   3rd Qu.:15.00     
    ##  Max.   :2023-06-30 00:00:00.00   Max.   :5.620   Max.   :20.00     
    ##                                                                     
    ##  plastic_bottles  polystyrene    cigarette_butts  glass_bottles   
    ##  Min.   :   0    Min.   :    0   Min.   :     0   Min.   :  0.00  
    ##  1st Qu.:1000    1st Qu.:  280   1st Qu.:  3200   1st Qu.: 10.00  
    ##  Median :1980    Median :  950   Median :  5500   Median : 18.00  
    ##  Mean   :2296    Mean   : 1631   Mean   : 15592   Mean   : 20.89  
    ##  3rd Qu.:2900    3rd Qu.: 2400   3rd Qu.: 16000   3rd Qu.: 28.00  
    ##  Max.   :9830    Max.   :11528   Max.   :310000   Max.   :110.00  
    ##  NA's   :1       NA's   :1       NA's   :1        NA's   :156     
    ##   plastic_bags      wrappers      sports_balls   homes_powered  
    ##  Min.   :    0   Min.   :  180   Min.   : 0.00   Min.   : 0.00  
    ##  1st Qu.:  280   1st Qu.:  840   1st Qu.: 6.00   1st Qu.:37.83  
    ##  Median :  680   Median : 1380   Median :11.00   Median :49.50  
    ##  Mean   : 1082   Mean   : 2330   Mean   :13.17   Mean   :45.87  
    ##  3rd Qu.: 1400   3rd Qu.: 2635   3rd Qu.:18.25   3rd Qu.:57.50  
    ##  Max.   :13450   Max.   :20100   Max.   :56.00   Max.   :93.67  
    ##  NA's   :1       NA's   :118     NA's   :261     NA's   :2      
    ##     robot          
    ##  Length:845        
    ##  Class :character  
    ##  Mode  :character  
    ##                    
    ##                    
    ##                    
    ## 

The dataframe `trash_wheels` contains observations for all 3 robots -
Mr. Trash Wheel, Professor Trash Wheel, and Gwynnda. The number of
observations in the `trash_wheels` dataframe is 845. Key variables
include the `dumpster` number, the `month`, and `year` of the `date`,
the total weight in tons collected for that time point in `weight_tons`
(e.g. the average weight collected from all robots is 3.0094793 tons)
and the total volume of cubic yards of trash collected for that time
point in `volume_cubic_yards`. More detailed information on the amount
of types of garbage collected for each time point is offered for
`plastic_bottles` (e.g. the total plastic bottles collected across all
robots and all time points is 1.938223^{6}), `polystyrene`,
`cigarette_butts`, `glass_bottles`, `plastic_bags`, `wrappers`, and
`sports_balls`. The total amount of homes powered by the collected trash
is available in `homes_powered`. As mentioned earlier, the `robot`
variable designates what robot provided the data for that observation.

## What was the total weight of trash collected by Professor Trash Wheel?

Professor Trash Wheel collected 216.26 tons of trash.

## What was the total number of cigarettes butts collected by Gwynnda in June 2022?

Gwynnda collected 1.812^{4} cigarette butts in June 2022.

# Question 3

`results` and `bakes` are in long format already, but all the other
files need to be pivoted to long format by pivoting the `series`
variables to be their own observations. The `series` variable after
pivoting is a character type, so we need to convert it to double.
Lastly, all datasets are sorted by `series` first and then `episode`
afterwards. Additionally, the `results` file also has some headers and
the data doesn’t start until the 3rd row. Therefore, we need the
`skip=2` option. For later joining, the `baker_name` variable in
`bakers` is separated into a variable for the baker’s first and last
name, called `baker` and `baker_last_name`, respectively. Rows with `NA`
in the `technical` or `result` variable for a given `series`/`episode`
combination indicate they were eliminated in the prior episode, so these
rows are omitted.

``` r
bakers <- read_csv("data/bakers.csv") |>
  janitor::clean_names() |>
  arrange(series) |>
  select(series, everything()) |>
  separate(baker_name, into = c("baker", "last_name"), sep = " ") |> 
  arrange(series, baker)
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
bakes <- read_csv("data/bakes.csv") |>
  janitor::clean_names() |>
  arrange(series, episode, baker)
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
results <- read_csv("data/results.csv", skip=2) |>
  janitor::clean_names() |>
  filter(!is.na(result)) |>
  arrange(series, episode)
```

    ## Rows: 1136 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (2): baker, result
    ## dbl (3): series, episode, technical
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
summary(bakers)
```

    ##      series        baker            last_name           baker_age    
    ##  Min.   : 1.0   Length:120         Length:120         Min.   :17.00  
    ##  1st Qu.: 3.0   Class :character   Class :character   1st Qu.:28.75  
    ##  Median : 6.0   Mode  :character   Mode  :character   Median :34.00  
    ##  Mean   : 5.6                                         Mean   :37.39  
    ##  3rd Qu.: 8.0                                         3rd Qu.:45.00  
    ##  Max.   :10.0                                         Max.   :71.00  
    ##  baker_occupation     hometown        
    ##  Length:120         Length:120        
    ##  Class :character   Class :character  
    ##  Mode  :character   Mode  :character  
    ##                                       
    ##                                       
    ## 

``` r
summary(bakes)
```

    ##      series        episode          baker           signature_bake    
    ##  Min.   :1.00   Min.   : 1.000   Length:548         Length:548        
    ##  1st Qu.:3.00   1st Qu.: 2.000   Class :character   Class :character  
    ##  Median :5.00   Median : 4.000   Mode  :character   Mode  :character  
    ##  Mean   :4.81   Mean   : 4.192                                        
    ##  3rd Qu.:7.00   3rd Qu.: 6.000                                        
    ##  Max.   :8.00   Max.   :10.000                                        
    ##  show_stopper      
    ##  Length:548        
    ##  Class :character  
    ##  Mode  :character  
    ##                    
    ##                    
    ## 

``` r
summary(results)
```

    ##      series          episode          baker             technical     
    ##  Min.   : 1.000   Min.   : 1.000   Length:710         Min.   : 1.000  
    ##  1st Qu.: 4.000   1st Qu.: 2.000   Class :character   1st Qu.: 2.000  
    ##  Median : 6.000   Median : 4.000   Mode  :character   Median : 4.000  
    ##  Mean   : 5.845   Mean   : 4.256                      Mean   : 4.843  
    ##  3rd Qu.: 8.000   3rd Qu.: 6.000                      3rd Qu.: 7.000  
    ##  Max.   :10.000   Max.   :10.000                      Max.   :13.000  
    ##                                                       NA's   :14      
    ##     result         
    ##  Length:710        
    ##  Class :character  
    ##  Mode  :character  
    ##                    
    ##                    
    ##                    
    ## 

To check completeness of each dataset, `anti_joins` are conducted
between all the datasets.

These datasets can all be joined on `series` and `baker` (with `episode`
sometimes for extra assurance). `gb_bakers` is created by doing a left
join on `bakers` with `bakes` first by `series` and `baker` as composite
primary keys. Then, `gb_bakers` is combined with `results` on `series`,
`episode`, and `baker` via a left join. Drop the `baker`-prefix from
variables to simplify their names, like `baker_age`. Variables are first
ordered by `series` and `episode`, then the baker names, and their
`result`, followed by more `baker`-related information variables. The
final dataset is then saved as a .csv in the `data` subfolder, called
`gb_bakers_data.csv`.

``` r
gb_bakers <- left_join(results, bakers, by = c("series", "baker")) |>
  left_join(bakes, by = c("series", "episode", "baker")) |>
  mutate(
    age = baker_age,
    occupation = baker_occupation
  ) |>
  select(
    series,
    episode,
    baker,
    last_name, 
    result,
    technical,
    everything()
  ) 

write_csv(gb_bakers, "data/gb_bakers_data.csv")
```

From this final dataset, to get a table of star baker or winners from
seasons 5-10, `filter` the rows accordingly, where `series` is in `5:10`
and `result` is either `WINNER` or `STAR BAKER`. Sort the table by
`series` and `episode` for readability.

``` r
stars_and_winners <- gb_bakers |>
  filter(
    (series %in% 5:10 & (result == "WINNER" | result == "STAR BAKER"))
  ) |>
  arrange(
    series,
    baker
  )
```

Some comments on the table:

- Season 5, Richard Burr got star baker 5, yet Nancy Birtwhistle won the
  whole series, having only gotten star baker 1 time prior, which was a
  surprise.
- Season 6 was a tossup since Nadiya Hussain won 3 star bakers, as did
  Ian Cumming.
- Season 7 was a close competition but Candice Brown just got 1 more
  star baker than others with 2 star bakers.
- Season 8 was a bit of a tossup, since Steven Carter-Bailey won 3 star
  bakers and Sophie Faldo won 2.
- Season 9 was also a close competition, with Rahul Mandal tied for the
  most star bakers with 2, along with Kim-Joy Hewlett with 2, and Ruby
  Bhogal with 2
- Season 10 was a huge upset, as David Atherton ultimately won without
  any star bakers, while Steph Blackwell by far won the most star bakers
  with 4

For the `viewers` data, the names have been cleaned with `clean_names`,
and the data was pivoted to longer format, turning each of the `series`
variables into a separate row like the other datasets. Rows with `NA`
for `viewers` are omitted for episodes that didn’t exist, e.g. S1E7. The
`series` data is mutated to `double` type to match the other `series`
variables, `series` and `episode` variables are moved to be the first
two variables, and then all the observations are sorted by `series` then
`episode`.

``` r
viewers <- read_csv("data/viewers.csv") |>
  janitor::clean_names() |>
  pivot_longer(
    cols = series_1:series_10,
    names_to = "series",
    names_prefix = "series_",
    values_to = "viewers"
  ) |>
  filter(!is.na(viewers)) |>
  mutate( 
    series = as.double(series) 
  ) |>
  select(
    series, episode, everything()
  ) |>
  arrange(series, episode)
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
head(viewers, n = 10)
```

    ## # A tibble: 10 × 3
    ##    series episode viewers
    ##     <dbl>   <dbl>   <dbl>
    ##  1      1       1    2.24
    ##  2      1       2    3   
    ##  3      1       3    3   
    ##  4      1       4    2.6 
    ##  5      1       5    3.03
    ##  6      1       6    2.75
    ##  7      2       1    3.1 
    ##  8      2       2    3.53
    ##  9      2       3    3.82
    ## 10      2       4    3.6

The average viewership in season 1 was 2.77 viewers. The average
viewership in season 5 was 10.0393 viewers.
